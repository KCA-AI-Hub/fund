from flask import Flask, render_template, request, jsonify, session
from openai import OpenAI
from dotenv import load_dotenv
import os
import uuid
from datetime import datetime
import logging
from logging.handlers import RotatingFileHandler
import json
import traceback
import requests
import pandas as pd
import re

# Load environment variables
load_dotenv()

# Configure logging
def setup_logging(app):
    """Configure logging for both file and console output"""
    # Create logs directory if it doesn't exist
    if not os.path.exists('logs'):
        os.makedirs('logs')
    
    # Set up log format
    log_format = logging.Formatter(
        '%(asctime)s [%(levelname)s] %(name)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # File handler with rotation
    file_handler = RotatingFileHandler(
        'logs/app.log',
        maxBytes=10 * 1024 * 1024,  # 10MB
        backupCount=10
    )
    file_handler.setFormatter(log_format)
    file_handler.setLevel(logging.INFO)
    
    # Console handler for development mode
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_format)
    console_handler.setLevel(logging.DEBUG if app.debug else logging.INFO)
    
    # Configure app logger
    app.logger.handlers.clear()  # Clear default handlers
    app.logger.addHandler(file_handler)
    app.logger.addHandler(console_handler)
    app.logger.setLevel(logging.DEBUG if app.debug else logging.INFO)
    
    # Configure werkzeug logger
    werkzeug_logger = logging.getLogger('werkzeug')
    werkzeug_logger.handlers.clear()
    werkzeug_logger.addHandler(file_handler)
    werkzeug_logger.addHandler(console_handler)
    werkzeug_logger.setLevel(logging.INFO)
    
    # Create separate logger for API calls
    api_logger = logging.getLogger('api_calls')
    api_file_handler = RotatingFileHandler(
        'logs/api_calls.log',
        maxBytes=10 * 1024 * 1024,  # 10MB
        backupCount=10
    )
    api_file_handler.setFormatter(log_format)
    api_logger.addHandler(api_file_handler)
    api_logger.addHandler(console_handler)
    api_logger.setLevel(logging.INFO)
    
    return api_logger

app = Flask(__name__)
app.config['SECRET_KEY'] = os.getenv('FLASK_SECRET_KEY', 'dev-secret-key')

# Setup logging
api_logger = setup_logging(app)

# Initialize OpenAI client
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
app.logger.info("OpenAI client initialized successfully")

# Dify Configuration
DIFY_API_URL = os.getenv('DIFY_API_URL', 'http://112.173.179.199:5001/v1')
DIFY_API_KEY = os.getenv('DIFY_API_KEY', '')
DIFY_DATASET_ID = os.getenv('DIFY_DATASET_ID', '')
AI_MODE = os.getenv('AI_MODE', 'dify')  # 'dify' or 'openai'
FALLBACK_TO_OPENAI = os.getenv('FALLBACK_TO_OPENAI', 'True').lower() == 'true'

app.logger.info(f"AI Mode: {AI_MODE}")
app.logger.info(f"Dify API URL: {DIFY_API_URL}")
app.logger.info(f"Dify Dataset ID: {DIFY_DATASET_ID}")
app.logger.info(f"Fallback to OpenAI: {FALLBACK_TO_OPENAI}")

# Store chat sessions in memory (in production, use a database)
chat_sessions = {}

# Load FAQ policy mapping from faq_topic.xlsx
faq_policy_map = {}
try:
    faq_file_path = os.path.join(os.path.dirname(__file__), 'data', 'faq_topic.xlsx')
    if os.path.exists(faq_file_path):
        faq_df = pd.read_excel(faq_file_path)
        faq_policy_map = dict(zip(faq_df['faq_id'], faq_df['policy_anchor']))
        app.logger.info(f"Loaded {len(faq_policy_map)} FAQ policy mappings from {faq_file_path}")
    else:
        app.logger.warning(f"FAQ file not found: {faq_file_path}")
except Exception as e:
    app.logger.error(f"Failed to load FAQ policy mapping: {e}")
    app.logger.error(traceback.format_exc())

# Request/Response logging middleware
@app.before_request
def log_request_info():
    """Log information about incoming requests"""
    app.logger.debug('Request Headers: %s', dict(request.headers))
    app.logger.info('Request: %s %s', request.method, request.path)
    if request.method in ['POST', 'PUT', 'PATCH']:
        if request.is_json:
            # Don't log sensitive data
            body = request.get_json()
            if body:
                safe_body = {k: v if k != 'api_key' else '***' for k, v in body.items()}
                app.logger.debug('Request Body: %s', json.dumps(safe_body, ensure_ascii=False)[:500])

@app.after_request
def log_response_info(response):
    """Log information about outgoing responses"""
    app.logger.info('Response: %s %s - Status: %s', 
                    request.method, request.path, response.status)
    return response

@app.route('/')
def index():
    """Render the main chat interface"""
    app.logger.info('Main page accessed from IP: %s', request.remote_addr)
    return render_template('index.html')

@app.route('/api/chat', methods=['POST'])
def chat():
    """Handle chat messages and generate responses"""
    session_id = None
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        session_id = data.get('session_id', str(uuid.uuid4()))
        prompt_template = data.get('prompt_template', None)  # ì„ íƒì  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

        app.logger.info(f'Chat request from session {session_id}: {user_message[:100]}...')
        api_logger.info(f'Session {session_id} - User message: {user_message}')

        # Initialize session if new
        if session_id not in chat_sessions:
            chat_sessions[session_id] = {
                'messages': [],
                'created_at': datetime.now()
            }

        # Add user message to session
        chat_sessions[session_id]['messages'].append({
            'role': 'user',
            'content': user_message
        })

        assistant_message = None
        retrieved_docs = []
        matched_faq_id = None
        related_laws = []

        # ===== Hybrid RAG Mode (Dify + Local Mapping) =====
        if AI_MODE == 'dify':
            try:
                app.logger.info('Using Hybrid RAG mode (Dify FAQ + Local Policy Mapping)')

                # STEP 1: Search FAQ in Dify Knowledge
                app.logger.info('Step 1: Searching FAQ in Dify Knowledge')
                faq_result = call_dify_knowledge(user_message, top_k=3)

                if faq_result['success'] and faq_result['records']:
                    retrieved_docs = faq_result['records']
                    app.logger.info(f'Retrieved {len(retrieved_docs)} FAQ records')

                    # STEP 2: Extract faq_id from best match
                    app.logger.info('Step 2: Extracting faq_id from best match')
                    best_faq = retrieved_docs[0]
                    faq_id = extract_faq_id_from_content(best_faq)

                    if faq_id:
                        app.logger.info(f'Extracted faq_id: {faq_id}')
                        matched_faq_id = faq_id

                        # STEP 3: Get policy_anchor from local mapping
                        app.logger.info('Step 3: Getting policy_anchor from local mapping')
                        policy_anchor = get_policy_anchor(faq_id)

                        if policy_anchor:
                            app.logger.info(f'Mapped policy_anchor: {policy_anchor[:100]}...')

                            # STEP 4: Search policy documents in Dify
                            app.logger.info('Step 4: Searching policy documents in Dify')
                            policy_docs = []
                            policy_anchors = [p.strip() for p in policy_anchor.split(';')]

                            for idx, anchor in enumerate(policy_anchors[:2], 1):  # Max 2 anchors
                                app.logger.debug(f'Searching policy doc {idx}: {anchor[:50]}...')
                                policy_result = call_dify_knowledge(anchor, top_k=2)
                                if policy_result['success'] and policy_result['records']:
                                    policy_docs.extend(policy_result['records'])
                                    app.logger.debug(f'Found {len(policy_result["records"])} policy docs')

                            app.logger.info(f'Total policy docs retrieved: {len(policy_docs)}')

                            # STEP 5: Generate answer with FAQ + Policy context
                            app.logger.info('Step 5: Generating answer with FAQ + Policy context')
                            assistant_message = generate_answer_with_context(
                                user_message,
                                retrieved_docs,
                                policy_docs
                            )

                            # STEP 6: Build related_laws from policy_anchor (NOT LLM generated!)
                            related_laws = []
                            for anchor in policy_anchors:
                                related_laws.append({
                                    'title': anchor.strip(),
                                    'source': 'FAQ Database',
                                    'faq_id': faq_id
                                })

                            app.logger.info(f'Hybrid RAG completed successfully for faq_id: {faq_id}')

                        else:
                            # No policy_anchor found, use FAQ only
                            app.logger.warning(f'No policy_anchor found for {faq_id}, using FAQ only')
                            assistant_message = generate_answer_with_context(
                                user_message,
                                retrieved_docs,
                                None
                            )

                    else:
                        # Could not extract faq_id, use basic RAG
                        app.logger.warning('Could not extract faq_id, using basic Dify RAG')
                        assistant_message = generate_answer_with_dify_rag(
                            user_message,
                            retrieved_docs,
                            prompt_template
                        )

                    app.logger.info(f'Answer generated for session {session_id}')

                # Fallback to OpenAI if Dify fails or no results
                elif FALLBACK_TO_OPENAI:
                    app.logger.warning('Dify FAQ search failed or no results, falling back to OpenAI')
                    assistant_message = generate_openai_response(session_id, user_message)
                else:
                    # No fallback, return error
                    raise Exception('Dify FAQ search failed and fallback is disabled')

            except Exception as e:
                app.logger.error(f'Error in Hybrid RAG mode: {str(e)}')
                app.logger.error(traceback.format_exc())
                if FALLBACK_TO_OPENAI:
                    app.logger.warning('Falling back to OpenAI due to error')
                    assistant_message = generate_openai_response(session_id, user_message)
                else:
                    raise

        # ===== OpenAI Direct Mode =====
        else:
            app.logger.info('Using OpenAI direct mode')
            assistant_message = generate_openai_response(session_id, user_message)

        # Add assistant message to session
        chat_sessions[session_id]['messages'].append({
            'role': 'assistant',
            'content': assistant_message
        })

        # Generate suggested answer
        suggested_answer = generate_suggested_answer(
            user_message,
            assistant_message,
            matched_faq_id=matched_faq_id,
            related_laws=related_laws
        )

        # Generate related_laws if not already set by Hybrid RAG
        if not related_laws:
            if AI_MODE == 'dify' and retrieved_docs:
                related_laws = extract_laws_from_retrieved_docs(retrieved_docs)
            else:
                related_laws = generate_related_laws(user_message)

        app.logger.info(f'Successfully processed chat request for session {session_id}')

        # Build response with metadata
        response_data = {
            'success': True,
            'message': assistant_message,
            'suggested_answer': suggested_answer,
            'related_laws': related_laws,
            'session_id': session_id,
            'metadata': {
                'ai_mode': AI_MODE,
                'retrieval_count': len(retrieved_docs) if retrieved_docs else 0,
                'matched_faq_id': matched_faq_id
            }
        }

        # Add retrieved documents info if available
        if retrieved_docs:
            response_data['metadata']['retrieved_docs'] = [
                {
                    'document_name': doc.get('segment', {}).get('document', {}).get('name', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                    'score': doc.get('score', 0),
                    'content_preview': doc.get('segment', {}).get('content', '')[:100] + '...'
                }
                for doc in retrieved_docs
            ]

        return jsonify(response_data)

    except Exception as e:
        error_session = session_id if session_id else 'unknown'
        app.logger.error(f'Error in chat endpoint for session {error_session}: {str(e)}')
        app.logger.error(f'Traceback: {traceback.format_exc()}')
        return jsonify({
            'success': False,
            'error': str(e),
            'ai_mode': AI_MODE
        }), 500

def generate_openai_response(session_id, user_message):
    """Generate response using OpenAI directly (without RAG)"""
    app.logger.debug('Calling OpenAI API for session %s', session_id)
    start_time = datetime.now()

    # Prepare messages for OpenAI API
    messages = [
        {'role': 'system', 'content': '''ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ê³µë¬´ì›ì´ ë¯¼ì›ì¸ì˜ ë¬¸ì˜ì— ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ê¸° ìœ„í•œ ê¸°ê¸ˆ ë¯¼ì›ì²˜ë¦¬ ì „ë¬¸ê°€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. í•­ìƒ ì •ì¤‘í•˜ê³  ê³µì†í•œ ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
2. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ ê·œì •ì„ ì¸ìš©í•  ë•ŒëŠ” ì •í™•í•œ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”
3. í•„ìš”í•œ ì„œë¥˜ë‚˜ ì ˆì°¨ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”
4. ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”'''}
    ]
    messages.extend(chat_sessions[session_id]['messages'])

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.7,
        max_tokens=1000
    )

    elapsed_time = (datetime.now() - start_time).total_seconds()
    api_logger.info(f'OpenAI API call completed in {elapsed_time:.2f}s - Tokens used: {response.usage.total_tokens}')

    assistant_message = response.choices[0].message.content
    app.logger.info(f'Generated response for session {session_id}: {assistant_message[:100]}...')

    return assistant_message

def extract_laws_from_retrieved_docs(retrieved_docs):
    """Extract related laws from Dify retrieved documents"""
    related_laws = []

    for doc in retrieved_docs:
        segment = doc.get('segment', {})
        content = segment.get('content', '')
        document_name = segment.get('document', {}).get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')
        score = doc.get('score', 0)

        related_laws.append({
            'title': document_name,
            'content': content[:300] + ('...' if len(content) > 300 else ''),
            'score': score,
            'source': 'Dify Knowledge'
        })

    return related_laws

def call_dify_knowledge(user_message, top_k=3):
    """
    Call Dify Knowledge API to retrieve relevant documents using RAG

    Args:
        user_message: User's query
        top_k: Number of top results to retrieve (default: 3)

    Returns:
        dict: Response containing retrieved documents and generated answer
    """
    try:
        app.logger.debug('Calling Dify Knowledge API')
        start_time = datetime.now()

        # Dify Knowledge Retrieve API endpoint
        url = f"{DIFY_API_URL}/datasets/{DIFY_DATASET_ID}/retrieve"

        headers = {
            'Authorization': f'Bearer {DIFY_API_KEY}',
            'Content-Type': 'application/json'
        }

        payload = {
            "query": user_message,
            "retrieval_model": {
                "search_method": "semantic_search",  # or "full_text_search", "hybrid_search"
                "reranking_enable": True,
                "reranking_model": {
                    "reranking_provider_name": "",
                    "reranking_model_name": ""
                },
                "top_k": top_k,
                "score_threshold_enabled": True,
                "score_threshold": 0.5
            }
        }

        app.logger.debug(f"Dify API Request URL: {url}")
        app.logger.debug(f"Dify API Request Payload: {json.dumps(payload, ensure_ascii=False)}")

        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()

        result = response.json()
        elapsed_time = (datetime.now() - start_time).total_seconds()

        # Extract retrieved documents
        records = result.get('records', [])
        app.logger.info(f"Dify Knowledge API call completed in {elapsed_time:.2f}s - Retrieved {len(records)} documents")
        api_logger.info(f"Dify RAG retrieved {len(records)} documents for query: {user_message[:100]}")

        # Log retrieved documents
        for idx, record in enumerate(records):
            score = record.get('score', 0)
            segment = record.get('segment', {})
            content_preview = segment.get('content', '')[:100]
            app.logger.debug(f"Document {idx+1} - Score: {score:.3f} - Content: {content_preview}...")

        return {
            'success': True,
            'records': records,
            'query': user_message,
            'elapsed_time': elapsed_time
        }

    except requests.exceptions.Timeout:
        app.logger.error('Dify API request timeout')
        return {
            'success': False,
            'error': 'Dify API timeout',
            'records': []
        }
    except requests.exceptions.RequestException as e:
        app.logger.error(f'Dify API request failed: {str(e)}')
        app.logger.error(f'Response: {e.response.text if hasattr(e, "response") else "No response"}')
        return {
            'success': False,
            'error': str(e),
            'records': []
        }
    except Exception as e:
        app.logger.error(f'Unexpected error calling Dify API: {str(e)}')
        app.logger.error(f'Traceback: {traceback.format_exc()}')
        return {
            'success': False,
            'error': str(e),
            'records': []
        }

def extract_faq_id_from_content(record):
    """
    Extract faq_id from Dify search result

    Args:
        record: Single record from Dify API response

    Returns:
        str: faq_id (e.g., "FAQ-í˜‘ì•½ì²´ê²°-0002") or None
    """
    try:
        content = record.get('segment', {}).get('content', '')

        # Method 1: Regex search for faq_id in CSV format
        # Content format: faq_id":"FAQ-í˜‘ì•½ì²´ê²°-0002";"question":"..."
        match = re.search(r'faq_id":"(.+?)"', content)
        if match:
            faq_id = match.group(1)
            app.logger.debug(f"Extracted faq_id from content: {faq_id}")
            return faq_id

        # Method 2: Check document name
        doc_name = record.get('segment', {}).get('document', {}).get('name', '')
        if doc_name and doc_name.startswith('FAQ-') and doc_name.endswith('.md'):
            faq_id = doc_name[:-3]  # Remove .md extension
            app.logger.debug(f"Extracted faq_id from document name: {faq_id}")
            return faq_id

        app.logger.warning("Could not extract faq_id from record")
        return None

    except Exception as e:
        app.logger.error(f"Error extracting faq_id: {e}")
        return None

def get_policy_anchor(faq_id):
    """
    Get policy_anchor from local mapping table

    Args:
        faq_id: FAQ identifier (e.g., "FAQ-í˜‘ì•½ì²´ê²°-0002")

    Returns:
        str: policy_anchor or None
    """
    if not faq_id:
        return None

    policy_anchor = faq_policy_map.get(faq_id)

    if policy_anchor:
        app.logger.debug(f"Mapped {faq_id} -> {policy_anchor[:50]}...")
    else:
        app.logger.warning(f"No policy_anchor found for faq_id: {faq_id}")

    return policy_anchor

def generate_answer_with_context(user_message, faq_records, policy_docs):
    """
    Generate answer using FAQ + Policy documents as context

    Args:
        user_message: User's question
        faq_records: FAQ records from Dify search
        policy_docs: Policy document records from Dify search (can be None/empty)

    Returns:
        str: Generated answer
    """
    try:
        # Build FAQ context
        faq_context = ""
        if faq_records:
            for idx, record in enumerate(faq_records[:2], 1):  # Top 2 FAQs
                content = record.get('segment', {}).get('content', '')
                # Extract question and answer from CSV format
                # Format: faq_id":"FAQ-...";"question":"ì§ˆë¬¸";"answer_text":"ë‹µë³€"
                question_match = re.search(r'question":"(.+?)"', content)
                answer_match = re.search(r'answer_text":"(.+?)"', content)

                if question_match and answer_match:
                    question = question_match.group(1)
                    answer = answer_match.group(1)
                    faq_context += f"\n[ì°¸ê³  FAQ {idx}]\nì§ˆë¬¸: {question}\në‹µë³€: {answer}\n"
                else:
                    # Fallback: use first 500 chars
                    faq_context += f"\n[ì°¸ê³  FAQ {idx}]\n{content[:500]}\n"

        # Build policy documents context
        policy_context = ""
        if policy_docs:
            for idx, doc in enumerate(policy_docs[:3], 1):  # Top 3 policy docs
                content = doc.get('segment', {}).get('content', '')
                policy_context += f"\n[ì°¸ê³  ë²•ë ¹ {idx}]\n{content[:500]}\n"

        # System prompt
        system_prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ICT ê¸°ê¸ˆì‚¬ì—… ë¯¼ì›ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

{faq_context}

{policy_context}

ë‹µë³€ ì‹œ ì§€ì¹¨:
1. ìœ„ ì°¸ê³  ìë£Œì˜ ë‚´ìš©ì„ ì •í™•íˆ í™œìš©í•˜ì„¸ìš”
2. ë²•ë ¹ì´ë‚˜ ê·œì •ì„ ì¸ìš©í•  ë•ŒëŠ” ì •í™•í•œ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”
3. í•„ìš”í•œ ì„œë¥˜ë‚˜ ì ˆì°¨ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”
4. ì°¸ê³  ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”
5. ì •ì¤‘í•˜ê³  ê³µì†í•œ ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"""

        # Call OpenAI API
        start_time = datetime.now()
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_message}
            ],
            temperature=0.7,
            max_tokens=1000
        )

        elapsed_time = (datetime.now() - start_time).total_seconds()
        assistant_message = response.choices[0].message.content

        api_logger.info(f'Answer generated in {elapsed_time:.2f}s - Tokens: {response.usage.total_tokens}')
        app.logger.debug(f'Generated answer: {assistant_message[:100]}...')

        return assistant_message

    except Exception as e:
        app.logger.error(f'Error generating answer with context: {str(e)}')
        app.logger.error(traceback.format_exc())
        raise

def generate_answer_with_dify_rag(user_message, retrieved_docs, prompt_template=None):
    """
    Generate answer using OpenAI with Dify retrieved documents as context

    Args:
        user_message: User's query
        retrieved_docs: Documents retrieved from Dify Knowledge
        prompt_template: Optional custom prompt template

    Returns:
        str: Generated answer
    """
    try:
        app.logger.debug('Generating answer with RAG context')

        # Build context from retrieved documents
        context_parts = []
        for idx, record in enumerate(retrieved_docs):
            segment = record.get('segment', {})
            content = segment.get('content', '')
            dataset_name = record.get('dataset_name', 'ë¬¸ì„œ')
            document_name = segment.get('document', {}).get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')
            score = record.get('score', 0)

            context_parts.append(f"""
[ì°¸ê³ ìë£Œ {idx+1}] (ê´€ë ¨ë„: {score:.2f})
ì¶œì²˜: {dataset_name} - {document_name}
ë‚´ìš©: {content}
""")

        context = "\n".join(context_parts)

        # Use custom prompt template or default
        if prompt_template:
            system_prompt = prompt_template
        else:
            system_prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ê³µë¬´ì›ì´ ë¯¼ì›ì¸ì˜ ë¬¸ì˜ì— ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ê¸° ìœ„í•œ ê¸°ê¸ˆ ë¯¼ì›ì²˜ë¦¬ ì „ë¬¸ê°€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. í•­ìƒ ì •ì¤‘í•˜ê³  ê³µì†í•œ ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
2. ìœ„ ì°¸ê³ ìë£Œì˜ ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ê³  í™œìš©í•˜ì„¸ìš”
3. ì°¸ê³ ìë£Œì— ëª…ì‹œëœ ë²•ë ¹ì´ë‚˜ ê·œì •ì´ ìˆë‹¤ë©´ ì •í™•í•œ ì¡°í•­ì„ ì¸ìš©í•˜ì„¸ìš”
4. í•„ìš”í•œ ì„œë¥˜ë‚˜ ì ˆì°¨ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”
5. ì°¸ê³ ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”
6. ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"""

        # Call OpenAI with RAG context
        start_time = datetime.now()

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_message}
            ],
            temperature=0.7,
            max_tokens=1000
        )

        elapsed_time = (datetime.now() - start_time).total_seconds()
        assistant_message = response.choices[0].message.content

        api_logger.info(f'RAG answer generated in {elapsed_time:.2f}s - Tokens: {response.usage.total_tokens}')

        return assistant_message

    except Exception as e:
        app.logger.error(f'Error generating RAG answer: {str(e)}')
        app.logger.error(f'Traceback: {traceback.format_exc()}')
        raise

def generate_suggested_answer(user_message, assistant_response, matched_faq_id=None, related_laws=None):
    """
    FAQ RAG ê¸°ë°˜ ë¯¼ì›ì²˜ë¦¬ ë‹µë³€ ìƒì„±

    Args:
        user_message: ì‚¬ìš©ì ì§ˆë¬¸
        assistant_response: ì±—ë´‡ ë‹µë³€ (FAQ RAG ê¸°ë°˜)
        matched_faq_id: ë§¤ì¹­ëœ FAQ ID (ì„ íƒ)
        related_laws: ê´€ë ¨ ë²•ë ¹ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ, ë³„ë„ íƒ­ì— í‘œì‹œë¨)

    Returns:
        str: HTML í˜•ì‹ì˜ ë¯¼ì›ì²˜ë¦¬ ë‹µë³€
    """
    try:
        app.logger.debug(f'Generating suggested answer (FAQ ID: {matched_faq_id})')

        # ========================================
        # ğŸ“ ë‹µë³€ì§€ì¹¨ë€ (ë‚˜ì¤‘ì— ìˆ˜ì • ê°€ëŠ¥)
        # ========================================
        system_instruction = """ë‹¹ì‹ ì€ ICT ê¸°ê¸ˆì‚¬ì—…ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê¸°ê¸ˆê·œì • ì—…ë¬´ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì§ì›ë“¤ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.
ì—…ë¬´ ì¡°ì–¸ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ë¬¸ì˜ì²˜ ì•ˆë‚´ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

âš ï¸ ì¤‘ìš” ê·œì¹™:
- ê´€ë ¨ ë²•ë ¹ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš” (ë³„ë„ íƒ­ì— í‘œì‹œë¨)
- ë¬¸ì˜ì²˜(ì „í™”ë²ˆí˜¸, ì´ë©”ì¼)ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ê¹”ë”í•˜ê³  ë³´ê¸° ì¢‹ì€ HTMLë¡œ ì‘ì„±í•˜ì„¸ìš”

ğŸ“‹ HTML í¬ë§· ì§€ì¹¨:
- <div class="answer-section">ë¡œ ê° ì„¹ì…˜ì„ ê°ì‹¸ì„¸ìš”
- ì œëª©ì€ <h4>ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: <h4>ğŸ“Œ í•µì‹¬ ë‹µë³€</h4>)
- ë³¸ë¬¸ì€ <p>ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
- ëª©ë¡ì€ <ul><li>ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
- ì ˆì°¨/ë‹¨ê³„ëŠ” <ol><li>ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
- ê°•ì¡°ëŠ” <strong>ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ì—¬ë°±ì„ ìœ„í•´ <br> ëŒ€ì‹  ë³„ë„ <p> íƒœê·¸ ì‚¬ìš©

ğŸ“ ê¶Œì¥ ì„¹ì…˜ êµ¬ì¡° (ìƒí™©ì— ë§ê²Œ ì„ íƒ):
- ğŸ“Œ í•µì‹¬ ë‹µë³€ / ê°œìš”
- ğŸ“ ì‘ì„± ë‚´ìš© / í•„ìˆ˜ í•­ëª© / í¬í•¨ë˜ì–´ì•¼ í•  ë‚´ìš©
- ğŸ“‹ í•„ìš” ì„œë¥˜ / ì¤€ë¹„ ì„œë¥˜
- ğŸ”„ ì²˜ë¦¬ ì ˆì°¨ / ì§„í–‰ ê³¼ì •
- â° ì²˜ë¦¬ ê¸°ê°„ / ì†Œìš” ì‹œê°„
- âš ï¸ ì£¼ì˜ì‚¬í•­ / ìœ ì˜ì‚¬í•­
- ğŸ’¡ ì°¸ê³ ì‚¬í•­ / ì¶”ê°€ ì •ë³´ (ê´€ë ¨ ì²˜ë¦¬ì§€ì¹¨ í¬í•¨)

âœ… ì¢‹ì€ ì˜ˆì‹œ:
<div class="answer-section">
    <h4>ğŸ“Œ í•µì‹¬ ë‹µë³€</h4>
    <p>ì‚¬ì—…ë¹„ êµë¶€ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.</p>
</div>

<div class="answer-section">
    <h4>ğŸ“ í•„ìš” ì„œë¥˜</h4>
    <ul>
        <li><strong>í˜‘ì•½ì„œ:</strong> ì‚¬ì—… í˜‘ì•½ ì²´ê²° í›„ ì œì¶œ</li>
        <li><strong>ê³„ì¢Œ ì‚¬ë³¸:</strong> ì‚¬ì—…ì ëª…ì˜ ê³„ì¢Œ</li>
    </ul>
</div>

<div class="answer-section">
    <h4>ğŸ”„ ì²˜ë¦¬ ì ˆì°¨</h4>
    <ol>
        <li>í˜‘ì•½ ì²´ê²° ë° ì„œë¥˜ ì œì¶œ</li>
        <li>ì„œë¥˜ ê²€í†  (3-5ì¼ ì†Œìš”)</li>
        <li>êµë¶€ ìŠ¹ì¸ ë° ì…ê¸ˆ</li>
    </ol>
</div>

<div class="answer-section">
    <h4>ğŸ’¡ ì°¸ê³ ì‚¬í•­</h4>
    <p>ê´€ë ¨ ì²˜ë¦¬ì§€ì¹¨: ICT ê¸°ê¸ˆì‚¬ì—… ìš´ì˜ì§€ì¹¨ ì œXXì¡°ì— ë”°ë¼ ì²˜ë¦¬ë©ë‹ˆë‹¤.</p>
    <p>ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¸ì˜í•˜ì„¸ìš”.</p>
</div>
"""
        # ========================================

        prompt = f"""{system_instruction}

ì§ˆë¬¸: {user_message}
ì°¸ê³  ë‹µë³€: {assistant_response}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ì› ì—…ë¬´ ì¡°ì–¸ í˜•ì‹ì˜ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ìœ„ì˜ HTML í¬ë§· ì§€ì¹¨ì„ ë”°ë¼ ê¹”ë”í•˜ê³  ë³´ê¸° ì¢‹ê²Œ êµ¬ì¡°í™”í•˜ì„¸ìš”.

âœ… í•„ìˆ˜ í¬í•¨ ì‚¬í•­:
- ì°¸ê³ ì‚¬í•­ ì„¹ì…˜ì— ê´€ë ¨ ì²˜ë¦¬ì§€ì¹¨(ìš´ì˜ì§€ì¹¨, ì‹œí–‰ì„¸ì¹™ ë“±)ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- ì²˜ë¦¬ì§€ì¹¨ì´ ì°¸ê³  ë‹µë³€ì— ìˆë‹¤ë©´ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì„¸ìš”

âš ï¸ ì¤‘ìš”: HTML ì½”ë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ```html``` ê°™ì€ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ íƒœê·¸ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{'role': 'user', 'content': prompt}],
            temperature=0.3,  # ë‚®ì¶°ì„œ ë” ì¼ê´€ì„± ìˆëŠ” ë‹µë³€
            max_tokens=800
        )

        api_logger.info(f'Suggested answer generated - Tokens used: {response.usage.total_tokens}')

        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```html, ``` ë“±)
        answer = response.choices[0].message.content
        answer = answer.replace('```html', '').replace('```', '').strip()

        app.logger.debug(f'Generated answer length: {len(answer)}')

        return answer

    except Exception as e:
        app.logger.error(f'Error generating suggested answer: {str(e)}')
        app.logger.error(traceback.format_exc())
        return "<p>ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>"

def generate_related_laws(user_message):
    """Generate related laws and regulations"""
    # Simplified version - in production, this would query a legal database
    common_laws = {
        'ê±´ì¶•': [
            {
                'title': 'ê±´ì¶•ë²•',
                'content': 'ì œ11ì¡° (ê±´ì¶•í—ˆê°€) ê±´ì¶•ë¬¼ì„ ê±´ì¶•í•˜ê±°ë‚˜ ëŒ€ìˆ˜ì„ í•˜ë ¤ëŠ” ìëŠ” íŠ¹ë³„ìì¹˜ì‹œì¥ã†íŠ¹ë³„ìì¹˜ë„ì§€ì‚¬ ë˜ëŠ” ì‹œì¥ã†êµ°ìˆ˜ã†êµ¬ì²­ì¥ì˜ í—ˆê°€ë¥¼ ë°›ì•„ì•¼ í•œë‹¤.'
            },
            {
                'title': 'ê±´ì¶•ë²• ì‹œí–‰ë ¹',
                'content': 'ì œ6ì¡° (ì ìš©ì˜ ì™„í™”) ê±´ì¶•ë¬¼ì˜ ëŒ€ì§€ê°€ ì§€ì—­ã†ì§€êµ¬ ë˜ëŠ” êµ¬ì—­ì— ê±¸ì¹˜ëŠ” ê²½ìš° ê·¸ ê±´ì¶•ë¬¼ê³¼ ëŒ€ì§€ì˜ ì „ë¶€ì— ëŒ€í•˜ì—¬ ëŒ€ì§€ì˜ ê³¼ë°˜ì´ ì†í•˜ëŠ” ì§€ì—­ã†ì§€êµ¬ ë˜ëŠ” êµ¬ì—­ ì•ˆì˜ ê±´ì¶•ë¬¼ ë° ëŒ€ì§€ì— ê´€í•œ ê·œì •ì„ ì ìš©í•œë‹¤.'
            }
        ],
        'ë„ë¡œ': [
            {
                'title': 'ë„ë¡œë²•',
                'content': 'ì œ61ì¡° (ë„ë¡œì ìš©í—ˆê°€) ë„ë¡œë¥¼ ì ìš©í•˜ë ¤ëŠ” ìëŠ” ë„ë¡œê´€ë¦¬ì²­ì˜ í—ˆê°€ë¥¼ ë°›ì•„ì•¼ í•œë‹¤.'
            }
        ],
        'í™˜ê²½': [
            {
                'title': 'í™˜ê²½ì •ì±…ê¸°ë³¸ë²•',
                'content': 'ì œ3ì¡° (ê¸°ë³¸ì´ë…) í™˜ê²½ì˜ ì§ˆì ì¸ í–¥ìƒê³¼ ê·¸ ë³´ì „ì„ í†µí•œ ì¾Œì í•œ í™˜ê²½ì˜ ì¡°ì„± ë° ì´ë¥¼ í†µí•œ ì¸ê°„ê³¼ í™˜ê²½ê°„ì˜ ì¡°í™”ì™€ ê· í˜•ì˜ ìœ ì§€ëŠ” êµ­ë¯¼ì˜ ê±´ê°•ê³¼ ë¬¸í™”ì ì¸ ìƒí™œì˜ í–¥ìœ  ë° êµ­í† ì˜ ë³´ì „ê³¼ í•­êµ¬ì ì¸ êµ­ê°€ë°œì „ì— í•„ìˆ˜ë¶ˆê°€ê²°í•œ ìš”ì†Œì„ì„ ì¸ì‹í•˜ê³ ...'
            }
        ]
    }
    
    # Simple keyword matching
    for keyword, laws in common_laws.items():
        if keyword in user_message:
            return laws
    
    # Default laws
    return [
        {
            'title': 'ë¯¼ì› ì²˜ë¦¬ì— ê´€í•œ ë²•ë¥ ',
            'content': 'ì œ9ì¡° (ë¯¼ì›ì˜ ì²˜ë¦¬ê¸°ê°„) í–‰ì •ê¸°ê´€ì˜ ì¥ì€ ë¯¼ì›ì˜ ì²˜ë¦¬ê¸°ê°„ì„ ì¢…ë¥˜ë³„ë¡œ ë¯¸ë¦¬ ì •í•˜ì—¬ ë¯¼ì›ì¸ì´ ì´ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ê²Œì‹œí•˜ê±°ë‚˜ ë¯¼ì›í¸ëŒì— ìˆ˜ë¡í•˜ëŠ” ë“±ì˜ ë°©ë²•ìœ¼ë¡œ ê³µí‘œí•˜ì—¬ì•¼ í•œë‹¤.'
        },
        {
            'title': 'í–‰ì •ì ˆì°¨ë²•',
            'content': 'ì œ17ì¡° (ì²˜ë¶„ì˜ ì‹ ì²­) í–‰ì •ì²­ì— ì²˜ë¶„ì„ êµ¬í•˜ëŠ” ì‹ ì²­ì€ ë¬¸ì„œë¡œ í•˜ì—¬ì•¼ í•œë‹¤. ë‹¤ë§Œ, ë‹¤ë¥¸ ë²•ë ¹ë“±ì— íŠ¹ë³„í•œ ê·œì •ì´ ìˆëŠ” ê²½ìš°ì™€ í–‰ì •ì²­ì´ ë¯¸ë¦¬ ë‹¤ë¥¸ ë°©ë²•ì„ ì •í•˜ì—¬ ê³µì‹œí•œ ê²½ìš°ì—ëŠ” ê·¸ëŸ¬í•˜ì§€ ì•„ë‹ˆí•˜ë‹¤.'
        }
    ]

@app.route('/api/new-session', methods=['POST'])
def new_session():
    """Create a new chat session"""
    session_id = str(uuid.uuid4())
    chat_sessions[session_id] = {
        'messages': [],
        'created_at': datetime.now()
    }
    app.logger.info(f'New session created: {session_id} from IP: {request.remote_addr}')
    return jsonify({'session_id': session_id})

@app.route('/api/chat/confirm', methods=['POST'])
def chat_confirm():
    """
    1ë‹¨ê³„: ì§ˆë¬¸ ìš”ì•½ ë° í™•ì¸
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì´í•´í•˜ê³  í™•ì¸ ë©”ì‹œì§€ë§Œ ìƒì„± (FAQ RAG í˜¸ì¶œ X)
    """
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        session_id = data.get('session_id', str(uuid.uuid4()))

        app.logger.info(f'[CONFIRM] Question from session {session_id}: {user_message[:100]}...')

        # ì§ˆë¬¸ ìš”ì•½/í™•ì¸ í”„ë¡¬í”„íŠ¸
        confirmation_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ê°„ë‹¨íˆ ìš”ì•½í•˜ì—¬ ë˜ë¬¼ì–´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë‹µë³€ í˜•ì‹:
"[ìš”ì•½ëœ ë‚´ìš©]ì— ëŒ€í•´ ë¬¸ì˜í•˜ì‹œëŠ” ê²ƒì´ ë§ë‚˜ìš”?"

ì˜ˆì‹œ:
- ì§ˆë¬¸: "ì‚¬ì—…ë¹„ êµë¶€ëŠ” ì–´ë–»ê²Œ ë°›ë‚˜ìš”?"
  â†’ "ì‚¬ì—…ë¹„ êµë¶€ ì ˆì°¨ì— ëŒ€í•´ ë¬¸ì˜í•˜ì‹œëŠ” ê²ƒì´ ë§ë‚˜ìš”?"
- ì§ˆë¬¸: "ì¸ê±´ë¹„ ê³„ì‚° ë°©ë²• ì•Œë ¤ì¤˜"
  â†’ "ì¸ê±´ë¹„ ì‚°ì • ë°©ë²•ì— ëŒ€í•´ ë¬¸ì˜í•˜ì‹œëŠ” ê²ƒì´ ë§ë‚˜ìš”?"
- ì§ˆë¬¸: "í˜‘ì•½ ì²´ê²° ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ”?"
  â†’ "í˜‘ì•½ ì²´ê²° ì‹œ í•„ìš”í•œ ì„œë¥˜ì— ëŒ€í•´ ë¬¸ì˜í•˜ì‹œëŠ” ê²ƒì´ ë§ë‚˜ìš”?"

ê°„ê²°í•˜ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""

        start_time = datetime.now()
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{'role': 'user', 'content': confirmation_prompt}],
            temperature=0.3,
            max_tokens=100
        )

        elapsed_time = (datetime.now() - start_time).total_seconds()
        confirmation_message = response.choices[0].message.content

        app.logger.info(f'[CONFIRM] Generated confirmation in {elapsed_time:.2f}s: {confirmation_message}')
        api_logger.info(f'Confirmation generated - Tokens: {response.usage.total_tokens}')

        return jsonify({
            'success': True,
            'message': confirmation_message,
            'session_id': session_id,
            'requires_confirmation': True
        })

    except Exception as e:
        app.logger.error(f'Error in confirmation: {str(e)}')
        app.logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    app.logger.info('='*50)
    app.logger.info('Starting Civil Complaint Chatbot Application')
    app.logger.info(f'Debug Mode: {app.debug}')
    app.logger.info(f'Log files location: logs/')
    app.logger.info('='*50)

    # í„°ë¯¸ë„ì— ëª…í™•í•˜ê²Œ URL ì¶œë ¥
    print('\n' + '='*60)
    print('ğŸš€ Civil Complaint Chatbot Server Started!')
    print('='*60)
    print(f'ğŸ“ Local:   http://localhost:5000')
    print(f'ğŸ“ Network: http://127.0.0.1:5000')
    print('='*60)
    print('Press CTRL+C to quit\n')

    app.run(debug=True, port=5000, host='127.0.0.1')